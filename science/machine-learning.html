<!DOCTYPE html>
<html lang="en-GB" xml:lang="en-GB" xmlns="http://www.w3.org/1999/xhtml" xmlns:dct="http://purl.org/dc/terms/" xmlns:cc="http://creativecommons.org/ns#">
<head>
  <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
  <!--meta charset="utf-8" /--><!-- Either this line or the above! http://www.w3.org/TR/html-polyglot/ -->
  <title>Machine Learning and Artificial Intelligence | Philosophy and Science</title>
  <link rel="stylesheet" type="text/css" media="screen" href="../css/s_124085_triad.css" title="Three-colour scheme" />
  <link rel="alternate stylesheet" type="text/css" media="screen" href="../css/systemcolours.css" title="System colours" />
  <link rel="alternate stylesheet" type="text/css" media="screen" href="../css/default.css" title="No colours" />
  <link rel="alternate stylesheet" type="text/css" media="screen" href="../css/meiert-qa.css" title="Meiert's QA Stylesheet" />
  <meta name="robots" content="noindex, nofollow" />
  <meta name="googlebot" content="noindex, nofollow" />
</head>
<body class="sciencepage">
  <header>
    <h1><span class="h1txt">Machine Learning and Artificial Intelligence</span></h1>
  </header>

  <nav>
    <h2>Navigation</h2>
    <ul>
      <li><a href="../index.html">Home</a></li>
      <li><a href="../philscience/index.html">Philosophy of Science</a>
        <ul>
          <li><a href="../philscience/fleck_ludwik.html" lang="de">Ludwik Fleck</a></li>
        </ul>
      </li>
      <li><a href="../philosophy/index.html">Philosophy</a>
        <ul>
          <li><a href="../philosophy/learningphilosophy.html">Learning Philosophy</a></li>
          <li><a href="../philosophy/thinking.html" title="Thinking, Including Critical Thinking">Thinking</a></li>
          <!--li><a href="../philosophy/religion.html">Religion and Faith</a></li-->
          <li><a href="../philosophy/hegel_gwf.html">G.W.F. Hegel</a></li>
        </ul>
      </li>
      <li><a href="../science/index.html">Science</a>
        <ul>
          <li><a href="../science/ioannidis.html" 
            title="Ioannidis on why most published research findings are false">John Ioannidis</a>
          </li>
          <li class="notyet"><a href="../science/mathematics.html">Mathematics &amp; Statistics</a></li>
          <li class="active"><a href="../science/machine-learning.html">Machine Learning</a></li>
          <li><a href="../science/doctoral-research.html">Dissertation / <abbr>PhD</abbr></a></li>
          <!--li><a href="../science/hdm_scienceday.html"><abbr>HdM</abbr> Science Day</a></li-->
        </ul>
      </li>
      <li><a href="../science/index.html">Humanities</a>
        <ul>
          <li><a href="../humanities/girard_rene.html" lang="fr">René Girard</a>
          </li>
        </ul>
      </li>
    </ul>
  </nav>

  <main>

    <h2>Biases in Machine Learning</h2>
    <p>Articles and links are listed in chronological order.</p>
    <ul>
      <li>Baldridge, Jason:
        <a href="https://techcrunch.com/2015/08/02/machine-learning-and-human-bias-an-uneasy-pair/">Machine Learning And Human Bias: An Uneasy Pair</a>,
        <i class="website">TechCrunch</i>, 02.08.2015.
      </li>
      <li>Bolukbasi, Tolga; Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam: 
        <a href="https://arxiv.org/abs/1607.06520">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a>,
        <i class="journal">arXiv.org</i>, 21.07.2016 (date of submission).
        <br />The first sentence of the abstract says,
        <q>The blind application of machine learning runs the risk of amplifying biases present in data.</q>
      </li>
      <li>Bornstein, Aaron M.:
        <a href="http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable">Is Artificial Intelligence Permanently Inscrutable?</a>,
        <i class="website">Nautilus</i>, 01.09.2016.
        Article about concerns relating to inscrutable neural networks (as opposed to rule-based systems).
      </li>
      <li>Rutkin, Aviva:
        <a href="https://www.newscientist.com/article/2115175-lazy-coders-are-training-artificial-intelligences-to-be-sexist/">Lazy coders are training artificial intelligences to be sexist</a>,
        <i class="magazine">New Scientist</i>, 05.12.2016.
      </li>
      <li>Townsend, Tess:
        <a href="https://www.recode.net/2017/1/18/14304964/data-facial-recognition-trouble-recognizing-black-white-faces-diversity">Most engineers are white — and so are the faces they use to train software</a>,
        <i class="website">Recode</i>, 18.01.2017.
        <br />Data sets for face recognition mainly use white faces, so they have problems recognising black faces.
      </li>
      <li>Devlin, Hannah:
        <a href="https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals"><abbr>AI</abbr> programs exhibit racial and gender biases, research reveals</a>,
        <i class="newspaper">The Guardian</i>, 13.04.2017.
      </li>
      <li>Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind:
        <a href="http://science.sciencemag.org/content/356/6334/183">Semantics derived automatically from language corpora contain human-like biases</a>,
        <i class="journal">Science</i>, <abbr>Vol.</abbr> 356, Issue 6334, 14 April 2017.
        <!--@@PDF-->
      </li>
      <li>Dunietz, Jesse: 
        <a href="http://nautil.us/blog/-the-fundamental-limits-of-machine-learning">The Fundamental Limits of Machine Learning</a>,
        <i class="magazine">Nautilus</i>, 14.08.2017.
      </li>
      <li>Simonite, Tom:
        <a href="https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women">Machines Taught by Photos Learn a Sexist View of Women</a>,
        <i class="magazine">Wired</i>, 21.08.2017.
      </li>
      <li>Violet Blue:
        <a href="https://www.engadget.com/2017/09/01/google-perspective-comment-ranking-system/">Google’s comment-ranking system will be a hit with the alt-right</a>,
        <i class="website">engadget</i>, 01.09.2017.
        The article's subtitle says, <q>The company's API for scoring toxicity in online discussions already behaves like a racist hand dryer.</q>
      </li>
      <li>Thompson, Andrew:
        <a href="https://motherboard.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias">Google’s Sentiment Analyzer Thinks Being Gay Is Bad</a>,
        <i class="website">Motherboard</i>, 25.10.2017.
      </li>
      <li>Caruana, Rich:
        <a href="https://www.ischool.berkeley.edu/events/2017/friends-dont-let-friends-deploy-black-box-models-importance-intelligibility-machine">Friends Don’t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning for Bias Detection and Prevention</a>,
        Berkeley School of Information, <abbr>UC</abbr> Berkeley, 08.11.2017. (Recording of a lecture; 90 minutes.)
      </li>
      <li>
        <a href="https://flipboard.com/@becomingdatasci/bias-in-machine-learning-rv7p7r9rz">Bias in machine learning: Articles about Bias in Machine Learning Algorithms / Data Science, and how to combat it</a>
        on Flipboard.com.
      </li>
      <!--
      <li>
        <a href=""></a>
      </li>
      -->
    </ul>

    <h2>Ethics and Oversight</h2>
    <ul>
      <li>Boer Deng:
        <a href="http://www.spektrum.de/news/koennen-wir-roboter-mit-ethik-erschaffen/1356774" hreflang="de" lang="de">Können wir Roboter mit Ethik erschaffen?</a>,
        <i class="magazine" lang="de">Spektrum</i>, 22.07.2015. (Translated from <i class="journal">Nature</i>.)
      </li>
      <li>Sample, Ian:
        <a href="https://www.theguardian.com/technology/2017/jan/27/ai-artificial-intelligence-watchdog-needed-to-prevent-discriminatory-automated-decisions"><abbr>AI</abbr> watchdog needed to regulate automated decision-making, say experts</a>,
        <i class="newspaper">The Guardian</i>, 27.01.2017.
      </li>
      <li>Buranyi, Stephen:
        <a href="https://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses">Rise of the racist robots – how <abbr>AI</abbr> is learning all our worst impulses</a>,
        <i class="newspaper">The Guardian</i>, 08.08.2017.
      </li>
      <li>O'Neil, Cathy:
        <a href="https://www.youtube.com/watch?v=_2u_eHHzRto">The era of blind faith in big data must end</a>
        <i class="conference"><!--@@CSS class--><abbr>TED</abbr></i> (YouTube, 13 minutes, 07.09.2017).
      </li>
      <li><abbr>AI</abbr> Now Institute:
        <a href="https://medium.com/@AINowInstitute/the-10-top-recommendations-for-the-ai-field-in-2017-b3253624a7">The 10 Top Recommendations for the <abbr>AI</abbr> Field in 2017</a>,
        <i class="blog"><abbr>AI</abbr> Now Institute on Medium</i>, 18.10.2017. Quote:
        <q>When a new drug is released into the marketplace, it must first undergo rigorous scientific trials and testing, and continued monitoring of its medium and long-term effects. Care and caution is paramount in this domain, because if things go wrong, many people experience significant harm. The same is true for <abbr>AI</abbr> systems in high stakes domains.</q>
        The first recommendation is,
        <q>Core public agencies, such as those responsible for criminal justice, healthcare, welfare, and education (<abbr>e.g.</abbr> “high stakes” domains) should no longer use ‘black box’ <abbr>AI</abbr> and algorithmic systems.</q>
        <br />This blog post is based on the
        <a href="https://ainowinstitute.org/AI_Now_2017_Report.pdf"><abbr>AI</abbr> Now Report 2017 (<abbr>PDF</abbr>)</a>.
      </li>
      <li lang="de">Beuth, Patrick:
        <a href="http://www.zeit.de/digital/internet/2017-10/kuenstliche-intelligenz-deepmind-back-box-regulierung/komplettansicht">Die Automaten brauchen Aufsicht</a>,
        <i class="newspaper">Zeit Online</i>, 25.10.2017.
      </li>
      <li>Sample, Ian:
        <a href="https://www.theguardian.com/science/2017/nov/01/artificial-intelligence-risks-gm-style-public-backlash-experts-warn">Artificial intelligence risks <abbr>GM</abbr>-style public backlash, experts warn</a>,
        <i class="newspaper">The Guardian</i>, 01.11.2017. Quote:
        <q>The white male dominance of the field has led to health apps that only cater for male bodies, photo services that labelled black people as gorillas and voice recognition systems that did not detect women’s voices.</q>
      </li>
      <li>Rosenberg, Scott:
        <a href="https://www.wired.com/story/why-ai-is-still-waiting-for-its-ethics-transplant/">Why Artificial Intelligence Is Still Waiting For Its Ethics Transplant</a>,
        <i class="magazine">Wired</i>, 01.11.2017.
      </li>
    </ul>

    <h2><abbr>AI</abbr> Safety</h2>
    <ul>
      <li>Krakovna, Victoriya:
        <a href="https://futureoflife.org/2016/02/29/introductory-resources-on-ai-safety-research/">Introductory Resources on <abbr>AI</abbr> Safety Research</a>,
        <i class="blog">Future of Life Institute blog</i>, 29.02.2016.
      </li>
      <li>Krakovna, Victoriya:
        <a href="https://vkrakovna.wordpress.com/2017/08/16/portfolio-approach-to-ai-safety-research/">Portfolio approach to <abbr>AI</abbr> safety research</a>,
        <i class="blog">Deep Safety</i>, 16.08.2017.
        (Also cross-posted to Zachary Lipton's blog
        <a href="http://approximatelycorrect.com/2017/08/18/portfolio-approach-to-ai-safety-research/">Approximately Correct</a>
        and the
        <a href="https://futureoflife.org/2017/08/17/portfolio-approach-to-ai-safety-research/">Future of Life Institute blog</a>.)
      </li>
    </ul>

    <h2>Unsorted Links</h2>
    <ul>
      <li>Leek, Jeff:
        <a href="https://simplystatistics.org/2014/05/07/why-big-data-is-in-trouble-they-forgot-about-applied-statistics/">Why big data is in trouble: they forgot about applied statistics</a>,
        <i class="blog">Simply Stats (blog)</i>, 07.05.2014.
      </li>
      <li>Lipton, Zachary Chase:
        <a href="https://www.kdnuggets.com/2015/04/model-interpretability-neural-networks-deep-learning.html">The Myth of Model Interpretability</a>,
        <i class="blog"><abbr>KD</abbr>Nuggets</i>, April 2015. (See also the update in Lipton's 2016 article, below.)
      </li>
      <li>Lipton, Zachary Chase:
        <a href="https://arxiv.org/abs/1606.03490">The Mythos of Model Interpretability</a>,
        presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (<abbr>WHI</abbr> 2016), New York, <abbr>NY</abbr>.
        <!--@@PDF-->
      </li>
      <li>Polonski, Vyacheslav W.:
        <a href="https://theconversation.com/how-artificial-intelligence-conquered-democracy-77675">How artificial intelligence conquered democracy</a>,
        <i class="website">The Conversation</i>, 08.08.2017 (Creative Commons licence).
        This article is about the use of artificial intelligence to target voters with specific messages.
      </li>
      <li>Scott Adams:
        <a href="http://dilbert.com/strip/2017-09-05">Dilbert, 05.09.2017</a>.
      </li>
      <li>Hall, Wendy; Pesenti, Jérôme:
        <a href="https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk">Growing the artificial intelligence industry in the <abbr>UK</abbr></a>,
        15.10.2017. This is an independent report commissioned by the <abbr>UK</abbr> Department for Digital, Culture, Media &amp; Sport and the Department for Business, Energy &amp; Industrial Strategy.
      </li>
      <li lang="de">Bayerische Landeszentrale für neue Medien (<abbr>BLM</abbr>):
        <a href="https://www.blm.de/infothek/magazin_tendenz/tendenz-2_2017.cfm">Tendenz, Ausgabe 02/17: Algorithmen</a>.
        <!--@@check!!-->
      </li>
      <li>Moore, Steve:
        <a href="https://www.kdnuggets.com/2017/08/ibm-top-10-machine-learning-use-cases-part1.html">Top 10 Machine Learning Use Cases: Part 1</a>,
        <i class="blog">KDNuggets blog</i>, August 2017.
        <!--@@what happened to the other parts?-->
      </li>
    </ul>

  </main>

  <footer>
    <p class="licence">
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img 
      alt="Creative Commons License" style="border-width:0" 
      src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />
      <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" 
      property="dct:title" rel="dct:type">Philosophy and Science</span> by 
      <a xmlns:cc="http://creativecommons.org/ns#" href="https://github.com/cstrobbe" property="cc:attributionName" 
      rel="cc:attributionURL">Christophe Strobbe</a> is licensed under a 
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" 
      lang="en-US">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
    </p>
    <p><a href="https://github.com/cstrobbe/philosophy-and-science">GitHub repository for
      <i>Philosophy and Science</i></a>.
    </p>
  </footer>
</body>
</html>
